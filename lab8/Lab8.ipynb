{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:51:52.205030Z",
     "start_time": "2019-05-13T14:51:52.198594Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from functional import seq\n",
    "from functional.streams import Sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:51:53.144300Z",
     "start_time": "2019-05-13T14:51:52.964877Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1180"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ustawy_dir = \"../lower_ustawy\"\n",
    "art_keyword = \"art\"\n",
    "ustawy_files = seq(os.listdir(ustawy_dir)).map(\n",
    "    lambda filename: open(ustawy_dir + \"/\" + filename).read())\n",
    "\n",
    "ustawy_files.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:51:53.303699Z",
     "start_time": "2019-05-13T14:51:53.296874Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "good, bad = ustawy_files.partition(lambda x: art_keyword in x[:2400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:51:53.695222Z",
     "start_time": "2019-05-13T14:51:53.685515Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1178"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:51:53.992111Z",
     "start_time": "2019-05-13T14:51:53.984143Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:51:54.291970Z",
     "start_time": "2019-05-13T14:51:54.283213Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'ustawa',\n",
       " 'z dnia 11 października 2013 r ',\n",
       " 'o wzajemnej pomocy przy dochodzeniu podatków  należności ',\n",
       " 'celnych i innych należności pieniężnych',\n",
       " '',\n",
       " '     ',\n",
       " 'font definitions   ',\n",
       " '  font face',\n",
       " '\\t font family helvetica ',\n",
       " '\\tpanose 1 2 11 5 4 2 2 2 2 2 4  ',\n",
       " ' font face',\n",
       " '\\t font family courier ',\n",
       " '\\tpanose 1 2 7 4 9 2 2 5 2 4 4  ',\n",
       " '']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad[0][:300].split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:51:54.592994Z",
     "start_time": "2019-05-13T14:51:54.586478Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', 'brak tekstu w postaci elektronicznej ', '']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad[1][:300].split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:51:54.843892Z",
     "start_time": "2019-05-13T14:51:54.840519Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def is_change(ustawa:str) -> bool:\n",
    "    return \"o zmianie ustawy\" in ustawa[:800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:51:55.046893Z",
     "start_time": "2019-05-13T14:51:55.033432Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "changes, not_changes = good.partition(is_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:51:55.239703Z",
     "start_time": "2019-05-13T14:51:55.234091Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Labeled =  namedtuple(\"Labeled\",\"text is_change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:51:55.458715Z",
     "start_time": "2019-05-13T14:51:55.452248Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def strip_title(text:str) -> str:\n",
    "    return text.split(art_keyword,maxsplit=1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:51:55.740230Z",
     "start_time": "2019-05-13T14:51:55.734134Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labeled_changes = changes.map(lambda txt: Labeled(text = strip_title(txt), is_change = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:51:55.967529Z",
     "start_time": "2019-05-13T14:51:55.964152Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labeled_not_changes = not_changes.map(lambda txt: Labeled(text = strip_title(txt), is_change = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:51:56.223822Z",
     "start_time": "2019-05-13T14:51:56.166513Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = (labeled_changes + labeled_not_changes).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:51:56.327859Z",
     "start_time": "2019-05-13T14:51:56.324394Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:51:57.133743Z",
     "start_time": "2019-05-13T14:51:57.124503Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#todo:bcm - use it or loose it\n",
    "def split(data):\n",
    "    data_len = len(data)\n",
    "    train_size = int(0.6 * data_len)\n",
    "    validation_size = int(0.2 * data_len)\n",
    "\n",
    "    train = data[:train_size]\n",
    "    validation = data[train_size:train_size + validation_size]\n",
    "    test = data[train_size + validation_size:]\n",
    "    return train,test,validation\n",
    "    \n",
    "# train, test,validation = split(data)\n",
    "# len(train),len(validation),len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:51:57.523195Z",
     "start_time": "2019-05-13T14:51:57.516265Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from random import choices # todo:bcm - take without replacements\n",
    "def full_selector(text):\n",
    "    return text\n",
    "\n",
    "def percentage_selector(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    take_count = int(0.1* len(lines))\n",
    "    return \"\\n\".join(choices(lines,k= take_count))\n",
    "\n",
    "def lines_selector(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    return \"\\n\".join(choices(lines,k= 10))\n",
    "    \n",
    "def line_selector(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines_len = len(lines)\n",
    "    return lines[random.randint(0,lines_len-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:51:57.802152Z",
     "start_time": "2019-05-13T14:51:57.799154Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Selector = namedtuple(\"Selector\", \"name selector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:51:58.042539Z",
     "start_time": "2019-05-13T14:51:58.035789Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selectors = seq([\n",
    "    Selector(name = \"full\", selector = full_selector),\n",
    "    Selector(name = \"percentage\", selector = percentage_selector),\n",
    "    Selector(name = \"lines\", selector = lines_selector),\n",
    "    Selector(name = \"line\", selector = line_selector),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:51:58.497075Z",
     "start_time": "2019-05-13T14:51:58.486805Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:51:59.170702Z",
     "start_time": "2019-05-13T14:51:59.158701Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jakaś', 'mój', 'mozna']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw_file = open(\"polish.stopwords.txt\")\n",
    "stop_words = set(word[:-1] for word in sw_file.readlines())\n",
    "sw_file.close()\n",
    "list(stop_words)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:51:59.497527Z",
     "start_time": "2019-05-13T14:51:59.492039Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def map_text(mapper, data):\n",
    "    return [Labeled(text = mapper(x.text), is_change= x.is_change) for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:51:59.997460Z",
     "start_time": "2019-05-13T14:51:59.987777Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def split2(x,y):\n",
    "    data_len = len(y)\n",
    "    train_size = int(0.6 * data_len)\n",
    "    validation_size = int(0.2 * data_len)\n",
    "\n",
    "    train = x[:train_size]\n",
    "    validation = x[train_size:train_size + validation_size]\n",
    "    test = x[train_size + validation_size:]\n",
    "    \n",
    "    y_train = y[:train_size]\n",
    "    y_validation = y[train_size:train_size + validation_size]\n",
    "    y_test = y[train_size + validation_size:]\n",
    "    \n",
    "    \n",
    "    return (train,test,validation), (y_train,y_test,y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:52:00.558795Z",
     "start_time": "2019-05-13T14:52:00.551090Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def grid_search(train_x, train_y,  parameters, pipeline):\n",
    "    grid_search_tune = GridSearchCV(\n",
    "        pipeline, parameters, cv=2, n_jobs=3, verbose=10, return_train_score =True)\n",
    "    grid_search_tune.fit(train_x, train_y)\n",
    "\n",
    "    \n",
    "    return (\n",
    "        grid_search_tune.best_estimator_,\n",
    "        grid_search_tune.best_params_,\n",
    "        grid_search_tune.cv_results_\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:52:00.793897Z",
     "start_time": "2019-05-13T14:52:00.784925Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(predicted,expected):\n",
    "    prec,recall,fbeta,support = precision_recall_fscore_support(y_true=expected,y_pred = predicted,average='weighted')\n",
    "   \n",
    "\n",
    "    print(\"F1 score: {}\".format(fbeta))\n",
    "    print(\"Precission: {}\".format(prec))\n",
    "    print(\"Recall: {}\".format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:52:01.072221Z",
     "start_time": "2019-05-13T14:52:01.065749Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def present_results(results,clf,val_x,val_y):\n",
    "    other = seq(range(0,2))\\\n",
    "    .flat_map(lambda i : [\"split{}_test_score\".format(i),\"split{}_train_score\".format(i)])\\\n",
    "    .to_list()\n",
    "    \n",
    "    labels = [\"mean_fit_time\",\"std_fit_time\",\"std_score_time\",\"mean_score_time\",\"params\",\n",
    "              \"std_test_score\",\"std_train_score\"] + other\n",
    "    \n",
    "    seq(labels).for_each(lambda label: results.pop(label))\n",
    "    frame = pd.DataFrame(results)\n",
    "    \n",
    "    display(\n",
    "        frame.sort_values(\"rank_test_score\",ascending = 1)\n",
    "    )\n",
    "    \n",
    "    print(\"On cross validation:\")\n",
    "    true_val = clf.predict(val_x)\n",
    "    evaluate(val_y,true_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:52:01.519838Z",
     "start_time": "2019-05-13T14:52:01.517444Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# xs,ys = [x.text for x in data], [x.is_change for x in data]\n",
    "# (tr_x,test_x,val_x),(tr_y,test_y,val_y)= split2(xs,ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:52:01.813640Z",
     "start_time": "2019-05-13T14:52:01.810863Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pipeline = Pipeline([\n",
    "#     ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "#     ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=3)),\n",
    "# ])\n",
    "# \n",
    "# parameters = {\n",
    "#     'tfidf__max_df': ( 0.25, 0.5,0.75,),\n",
    "#     'tfidf__ngram_range': [(1,2),(1, 3)],\n",
    "#     \"clf__estimator__C\": [0.1,0.15,0.2,0.25,0.3],\n",
    "# }\n",
    "\n",
    "# best_clf, best_params, results  = grid_search(tr_x + test_x, tr_y+ test_y, parameters, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:52:02.256281Z",
     "start_time": "2019-05-13T14:52:02.253527Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:52:03.281466Z",
     "start_time": "2019-05-13T14:52:03.272407Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_linear_svc(data, params,selector:Selector):\n",
    "    mapped_data =  map_text(selector.selector,data)\n",
    "    xs,ys = [x.text for x in mapped_data], [x.is_change for x in mapped_data]\n",
    "    (tr_x,test_x,val_x),(tr_y,test_y,val_y)= split2(xs,ys)\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "        ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=3)),\n",
    "    ])\n",
    "    \n",
    "    display(HTML(\"<h2>Selector: {}</h2>\".format(selector.name)))\n",
    "    \n",
    "    best_clf, best_params, results  = grid_search(tr_x + test_x, tr_y+ test_y, parameters, pipeline)\n",
    " #todo:bcm - resutls are the same :XXXX\n",
    "    print(best_params)\n",
    "    present_results(results,best_clf,val_x,val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:52:04.064828Z",
     "start_time": "2019-05-13T14:52:04.059458Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'tfidf__max_df': ( 0.25, 0.5,0.75,),\n",
    "    'tfidf__ngram_range': [(1,2),(1, 3)],\n",
    "    \"clf__estimator__C\": [0.1,0.2,0.25,0.3],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:52:05.572124Z",
     "start_time": "2019-05-13T14:52:05.562617Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-cfc8ee8cb8f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m \u001b[0;31m# To avoid loosing results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raise Exception # To avoid loosing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:55:15.258298Z",
     "start_time": "2019-05-13T14:52:07.913174Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Selector: full</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:   49.2s\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:  2.5min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-424722341d0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mselectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_each\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mevaluate_linear_svc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/functional/pipeline.py\u001b[0m in \u001b[0;36mfor_each\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-424722341d0a>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(selector)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mselectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_each\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mevaluate_linear_svc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-62-737824778f34>\u001b[0m in \u001b[0;36mevaluate_linear_svc\u001b[0;34m(data, params, selector)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<h2>Selector: {}</h2>\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mbest_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_y\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m  \u001b[0;31m#todo:bcm - resutls are the same :XXXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-854826c7178b>\u001b[0m in \u001b[0;36mgrid_search\u001b[0;34m(train_x, train_y, parameters, pipeline)\u001b[0m\n\u001b[1;32m      2\u001b[0m     grid_search_tune = GridSearchCV(\n\u001b[1;32m      3\u001b[0m         pipeline, parameters, cv=2, n_jobs=3, verbose=10, return_train_score =True)\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mgrid_search_tune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "selectors.for_each(lambda selector: evaluate_linear_svc(data,parameters,selector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.735491Z",
     "start_time": "2019-05-13T14:49:52.598Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raise Exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Installation:\n",
    "    >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.737755Z",
     "start_time": "2019-05-13T14:49:52.638Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.742408Z",
     "start_time": "2019-05-13T14:49:52.641Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oneline_data = map_text(lambda text: text.replace(\"\\n\", \" \"),data)\n",
    "# oneline_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.745308Z",
     "start_time": "2019-05-13T14:49:52.644Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def to_fast_text(x: Labeled)-> str :\n",
    "    label = \"1\" if x.is_change else \"0\"\n",
    "    replaced = x.text.replace(\"\\\"\",\"\\\"\\\"\")\n",
    "    return \"__label__{}, \\\"{}\\\"\\n\".format(label,replaced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.747277Z",
     "start_time": "2019-05-13T14:49:52.646Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_len = len(oneline_data)\n",
    "train_size = int(0.6 * data_len)\n",
    "validation_size = int(0.2 * data_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.751811Z",
     "start_time": "2019-05-13T14:49:52.649Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ol_train = oneline_data[:train_size]\n",
    "ol_test = oneline_data[train_size:train_size+ validation_size]\n",
    "ol_val = oneline_data[validation_size+ train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.755518Z",
     "start_time": "2019-05-13T14:49:52.651Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ol_to_file(name:str, ol:List[Labeled])-> None:\n",
    "    file = open(\"data.bak/{}\".format(name),\"w\")\n",
    "\n",
    "    for t in ol:\n",
    "        file.write(to_fast_text(t))\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.760934Z",
     "start_time": "2019-05-13T14:49:52.654Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ol_to_file(\"train.csv\",ol_train)\n",
    "ol_to_file(\"test.csv\",ol_test)\n",
    "ol_to_file(\"val.csv\",ol_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.763373Z",
     "start_time": "2019-05-13T14:49:52.656Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = fastText.train_supervised(\"data.bak/train.csv\",lr=0.7,epoch = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.767086Z",
     "start_time": "2019-05-13T14:49:52.659Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_result = model.test(\"data.bak/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.769231Z",
     "start_time": "2019-05-13T14:49:52.664Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO:bcm - compute my own precision\n",
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.771177Z",
     "start_time": "2019-05-13T14:49:52.668Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ROOT_URL = 'https://s3.eu-central-1.amazonaws.com/borchmann'\n",
    "FORWARD_FILE = 'cse/lm-polish-forward-v0.2.pt'\n",
    "BACKWARD_FILE = 'cse/lm-polish-backward-v0.2.pt'\n",
    "GLOVE_FILE = 'glove/poleval.txt'\n",
    "\n",
    "FORWARD_LM = f'{ROOT_URL}/{FORWARD_FILE}'\n",
    "BACKWARD_LM = f'{ROOT_URL}/{BACKWARD_FILE}'\n",
    "GLOVE = f'{ROOT_URL}/{GLOVE_FILE}'\n",
    "\n",
    "print(FORWARD_LM)\n",
    "print(BACKWARD_LM)\n",
    "print(GLOVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.774702Z",
     "start_time": "2019-05-13T14:49:52.669Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! wget https://s3.eu-central-1.amazonaws.com/borchmann/cse/lm-polish-forward-v0.2.pt -O data.bak/polish_forward.pt\n",
    "# ! wget https://s3.eu-central-1.amazonaws.com/borchmann/cse/lm-polish-backward-v0.2.pt -O data.bak/polish_backward.pt\n",
    "# ! wget https://s3.eu-central-1.amazonaws.com/borchmann/glove/poleval.txt -O data.bak/glove.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.777448Z",
     "start_time": "2019-05-13T14:49:52.674Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.778750Z",
     "start_time": "2019-05-13T14:49:52.676Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.embeddings import StackedEmbeddings, CharLMEmbeddings, TokenEmbeddings   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.780173Z",
     "start_time": "2019-05-13T14:49:52.678Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# using #https://github.com/applicaai/poleval-2018.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.785188Z",
     "start_time": "2019-05-13T14:49:52.681Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from flair.embeddings import TokenEmbeddings\n",
    "from flair.data import Sentence\n",
    "from typing import List\n",
    "from gensim.models import KeyedVectors\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class KeyedWordEmbeddings(TokenEmbeddings):\n",
    "    def __init__(self, embeddings):\n",
    "        self.name = embeddings\n",
    "        self.static_embeddings = True\n",
    "        self.precomputed_word_embeddings = KeyedVectors.load_word2vec_format(embeddings)\n",
    "        self.known_words = set(self.precomputed_word_embeddings.index2word)\n",
    "        self.__embedding_length: int = self.precomputed_word_embeddings.vector_size\n",
    "        super().__init__()\n",
    "\n",
    "    @property\n",
    "    def embedding_length(self) -> int:\n",
    "        return self.__embedding_length\n",
    "\n",
    "    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            for token, token_idx in zip(sentence.tokens, range(len(sentence.tokens))):\n",
    "                token: Token = token\n",
    "                if token.text in self.known_words:\n",
    "                    word_embedding = self.precomputed_word_embeddings[token.text]\n",
    "                elif token.text.lower() in self.known_words:\n",
    "                    word_embedding = self.precomputed_word_embeddings[token.text.lower()]\n",
    "                else:\n",
    "                    word_embedding = self.precomputed_word_embeddings['<unk>']\n",
    "                word_embedding = torch.FloatTensor(word_embedding)\n",
    "                token.set_embedding(self.name, word_embedding)\n",
    "        return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.786886Z",
     "start_time": "2019-05-13T14:49:52.684Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# embedding_types: List[TokenEmbeddings] = [\n",
    "#     KeyedWordEmbeddings(GLOVE),\n",
    "#     CharLMEmbeddings(FORWARD_LM),\n",
    "#     CharLMEmbeddings(BACKWARD_LM)\n",
    "# ]\n",
    "\n",
    "# embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "#taskes 4 ever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.792565Z",
     "start_time": "2019-05-13T14:49:52.689Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# corpus = NLPTaskDataFetcher.load_classification_corpus(\n",
    "#     Path('./data.bak'),\n",
    "#     test_file='test.csv',\n",
    "#     dev_file='val.csv',\n",
    "#     train_file='train.csv'\n",
    "# )\n",
    "\n",
    "# document_embeddings = DocumentLSTMEmbeddings(\n",
    "#     embeddings,\n",
    "#     hidden_size=512,\n",
    "#     reproject_words=True,\n",
    "#     reproject_words_dimension=256\n",
    "# )\n",
    "\n",
    "# classifier = TextClassifier(\n",
    "#     document_embeddings,\n",
    "#     label_dictionary=corpus.make_label_dictionary(),\n",
    "#     multi_label=False\n",
    "# )\n",
    "\n",
    "# trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "# trainer.train('./data.', max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.795829Z",
     "start_time": "2019-05-13T14:49:52.691Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "\" \" + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.799275Z",
     "start_time": "2019-05-13T14:49:52.696Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "from pathlib import Path\n",
    "from flair.data import TaggedCorpus\n",
    "\n",
    "# use your own data path\n",
    "data_folder = Path('./data.bak')\n",
    "\n",
    "# load corpus containing training, test and dev data\n",
    "corpus: TaggedCorpus = NLPTaskDataFetcher.load_classification_corpus(data_folder,\n",
    "                                                                     test_file='test.csv',\n",
    "                                                                     dev_file='val.csv',\n",
    "                                                                     train_file='train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.801399Z",
     "start_time": "2019-05-13T14:49:52.699Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import wrapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.804129Z",
     "start_time": "2019-05-13T14:49:52.702Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(wrapt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.808228Z",
     "start_time": "2019-05-13T14:49:52.705Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from flair.data import TaggedCorpus\n",
    "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.810769Z",
     "start_time": "2019-05-13T14:49:52.706Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4. initialize embeddings\n",
    "word_embeddings = [WordEmbeddings('pl'),\n",
    "                   # comment in flair embeddings for state-of-the-art results\n",
    "#                    FlairEmbeddings('polish-forward'),\n",
    "                   # FlairEmbeddings('polish-backward'),\n",
    "                   ]\n",
    "\n",
    "\n",
    "document_embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(word_embeddings,\n",
    "                                                                     hidden_size=512,\n",
    "                                                                     reproject_words=True,\n",
    "                                                                     reproject_words_dimension=256,\n",
    "                                                                     ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.813352Z",
     "start_time": "2019-05-13T14:49:52.708Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_dict = corpus.make_label_dictionary()\n",
    "len(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.816623Z",
     "start_time": "2019-05-13T14:49:52.711Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 5. initialize sequence tagger\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "#                                         embeddings=embeddings,\n",
    "#                                         tag_dictionary=tag_dictionary,\n",
    "#                                         tag_type=tag_type,\n",
    "#                                         use_crf=True)\n",
    "\n",
    "# 6. initialize trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T14:49:55.818107Z",
     "start_time": "2019-05-13T14:49:52.714Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 5. create the text classifier\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict, multi_label=False)\n",
    "\n",
    "# 6. initialize the text classifier trainer\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "# 7. start the training\n",
    "trainer.train('resources/taggers/ag_news',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              anneal_factor=0.5,\n",
    "              patience=5,\n",
    "              max_epochs=150)\n",
    "\n",
    "# 8. plot training curves (optional)\n",
    "from flair.visual.training_curves import Plotter\n",
    "plotter = Plotter()\n",
    "plotter.plot_training_curves('resources/taggers/ag_news/loss.tsv')\n",
    "plotter.plot_weights('resources/taggers/ag_news/weights.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T12:01:31.166239Z",
     "start_time": "2019-05-13T12:01:31.153159Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
