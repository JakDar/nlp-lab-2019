{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!#git clone -b poleval/task3 https://github.com/n-waves/fastai --depth 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T08:46:44.401958Z",
     "start_time": "2019-05-20T08:46:44.398089Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import namedtuple\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T08:46:44.812603Z",
     "start_time": "2019-05-20T08:46:44.777391Z"
    }
   },
   "outputs": [],
   "source": [
    "from functional import seq, pseq\n",
    "from functional.streams import Sequence\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "def display_seq(sequence:Sequence,rows:int)-> None:\n",
    "        sequence._repr_html_= lambda :sequence.tabulate(rows,tablefmt='html')\n",
    "        display(sequence)\n",
    "        sequence._repr_html_= lambda :sequence.tabulate(10,tablefmt='html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T08:46:45.616900Z",
     "start_time": "2019-05-20T08:46:45.152187Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T08:46:46.091260Z",
     "start_time": "2019-05-20T08:46:46.079542Z"
    }
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "def compose(*functions):\n",
    "    def compose2(f, g):\n",
    "        return lambda x: f(g(x))\n",
    "    return functools.reduce(compose2, functions, lambda x: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T08:45:09.261251Z",
     "start_time": "2019-05-20T08:45:09.258254Z"
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mblobs.bak/work/up_low50k/\u001b[00m\r\n",
      "├── \u001b[01;34mmodels\u001b[00m\r\n",
      "│   └── fwd_v50k_finetune_lm_enc.h5\r\n",
      "└── \u001b[01;34mtmp\u001b[00m\r\n",
      "    ├── sp-50k.model\r\n",
      "    └── sp-50k.vocab\r\n",
      "\r\n",
      "2 directories, 3 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree blobs.bak/work/up_low50k/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai_model_path = \"./blobs.bak/work/up_low50k/models/fwd_v50k_finetune_lm_enc.h5\"\n",
    "sentencepiece_model_path = \"./blobs.bak/work/up_low50k/tmp/sp-50k.model\"\n",
    "sentencepiece_vocab_path = \"./blobs.bak/work/up_low50k/tmp/sp-50k.vocab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists_or_ex(path):\n",
    "    file = open(path,\"r\")\n",
    "    file.close()\n",
    "_ = [exists_or_ex(path) for path in [fastai_model_path, sentencepiece_model_path, sentencepiece_vocab_path]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "spm_processor = spm.SentencePieceProcessor()\n",
    "spm_processor.Load(sentencepiece_model_path)\n",
    "\n",
    "spm_processor.SetEncodeExtraOptions(\"bos:eos\")\n",
    "spm_processor.SetDecodeExtraOptions(\"bos:eos\")\n",
    "# spm_processor.LoadVocabulary(sentencepiece_vocab_path,threshold= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spm_processor.encode_as_pieces(\"Ala ma kota i kot alę\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'<s>', b'\\xe2\\x96\\x81New', b'\\xe2\\x96\\x81Y', b'o', b'rk', b'</s>']\n",
      "[b'<s>', b'\\xe2\\x96\\x81New', b'\\xe2\\x96\\x81Yo', b'rk', b'</s>']\n",
      "[b'<s>', b'\\xe2\\x96\\x81New', b'\\xe2\\x96\\x81Y', b'or', b'k', b'</s>']\n",
      "[b'<s>', b'\\xe2\\x96\\x81New', b'\\xe2\\x96\\x81Yo', b'rk', b'</s>']\n",
      "[b'<s>', b'\\xe2\\x96\\x81N', b'e', b'w', b'\\xe2\\x96\\x81York', b'</s>']\n"
     ]
    }
   ],
   "source": [
    "for n in range(5):\n",
    "    print(spm_processor.SampleEncodeAsPieces('New York', -1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: invalid number of lines: ‘./blobs.bak/’\r\n"
     ]
    }
   ],
   "source": [
    "! head -n ./blobs.bak/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wgts = torch.load(fastai_model_path, map_location=lambda storage, loc: loc)\n",
    "wgts = torch.load(fastai_model_path, map_location='cuda:0')\n",
    "# enc_wgts = to_np(wgts['0.encoder.weight'])\n",
    "# row_m = enc_wgts.mean(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lm(bptt, max_seq, n_tok, emb_sz, n_hid, n_layers, pad_token, bidir=False,\n",
    "           tie_weights=True, qrnn=False):\n",
    "    rnn_enc = MultiBatchRNN(bptt, max_seq, n_tok, emb_sz, n_hid, n_layers, pad_token=pad_token, bidir=bidir, qrnn=qrnn)\n",
    "    enc = rnn_enc.encoder if tie_weights else None\n",
    "    return SequentialRNN(rnn_enc, LinearDecoder(n_tok, emb_sz, 0, tie_encoder=enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): MultiBatchRNN(\n",
       "    (encoder): Embedding(50000, 400, padding_idx=1)\n",
       "    (encoder_with_dropout): EmbeddingDropout(\n",
       "      (embed): Embedding(50000, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDrop(\n",
       "        (module): LSTM(400, 1150)\n",
       "      )\n",
       "      (1): WeightDrop(\n",
       "        (module): LSTM(1150, 1150)\n",
       "      )\n",
       "      (2): WeightDrop(\n",
       "        (module): LSTM(1150, 1150)\n",
       "      )\n",
       "      (3): WeightDrop(\n",
       "        (module): LSTM(1150, 400)\n",
       "      )\n",
       "    )\n",
       "    (dropouti): LockedDropout()\n",
       "    (dropouths): ModuleList(\n",
       "      (0): LockedDropout()\n",
       "      (1): LockedDropout()\n",
       "      (2): LockedDropout()\n",
       "      (3): LockedDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=50000, bias=False)\n",
       "    (dropout): LockedDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enc_wgts = to_np(wgts['encoder.weight']) #I think it expects  wgts top have 0.endoder.weight\n",
    "# row_m = enc_wgts.mean(0)\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "UNK_ID = 0\n",
    "PAD_ID = 1\n",
    "BOS_ID = 2\n",
    "EOS_ID = 3\n",
    "UP_ID  = 4\n",
    "bs=22\n",
    "\n",
    "lm_path=  fastai_model_path\n",
    "\n",
    "# em_sz,nh,nl = 400,1150,3  # that should be the case\n",
    "em_sz,nh,nl = 400 ,1150,4\n",
    "\n",
    "bptt=5\n",
    "vs = len(spm_processor)\n",
    "\n",
    "lm = get_lm(bptt, 1000000, vs, em_sz, nh, nl, PAD_ID)\n",
    "lm = to_gpu(lm)\n",
    "load_model(lm[0],lm_path)\n",
    "lm.reset()\n",
    "lm.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMTextDataset(Dataset):\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.x[idx]\n",
    "        return sentence[:-1], sentence[1:]\n",
    "\n",
    "    def __len__(self): return len(self.x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data,vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([    2,   618,     8,    91,   431, 11111,   344,     3])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inpucik = \"Mama bardzo dobrze wydała swoje\"\n",
    "test_ids = [np.array(spm_processor.encode_as_ids(inpucik))]\n",
    "test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mama bardzo dobrze wydała swoje'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = LMTextDataset(test_ids)\n",
    "test_samp = SortSampler(test_ids, key=lambda x: len(test_ids[x]))\n",
    "test_dl = DataLoader(test_ds, bs, transpose=True, transpose_y=True, num_workers=1, pad_idx=PAD_ID, sampler=test_samp, pre_pad=False)\n",
    "x,y = list(test_dl)[0]\n",
    "x2 = [int(i) for i in x]\n",
    "# y2 = [int(i) for i in y]\n",
    "spm_processor.decode_ids(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word(ss,model,toks_at_once):\n",
    "    ids = spm_processor.encode_as_ids(ss)\n",
    "    return spm_processor.decode_ids(\n",
    "        next_tokens(ids,model,toks_at_once)\n",
    "    )\n",
    "\n",
    "def next_tokens(ids_,model,toks_at_once): #toks_at_once wont be used\n",
    "    ids = [np.array(ids_)]\n",
    "    test_ds = LMTextDataset(ids)\n",
    "    test_samp = SortSampler(ids, key=lambda x: len(test_ids[x]))\n",
    "    dl = DataLoader(test_ds, bs, transpose=True, transpose_y=True, num_workers=1, pad_idx=PAD_ID, sampler=test_samp, pre_pad=False)\n",
    "    \n",
    "    tensor1 = None\n",
    "    model.reset() # todo:bcm - do or dont'?\n",
    "    with no_grad_context():\n",
    "        for (x, y) in dl:\n",
    "            tensor1= model(x)\n",
    "    p= tensor1[0]\n",
    "    print(p.size())\n",
    "#     r = torch.multinomial(p[-1].exp(), toks_at_once)\n",
    "    r = [seq(i for i in p[-1]).map(lambda x: float(x)).enumerate().max_by(lambda id_val: id_val[1])[0]]\n",
    "    \n",
    "    predicted_ids = [int(r_) for r_ in r]\n",
    "    return predicted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def next_words_stm(ss,lm,pred_len, input_tokens = 8):\n",
    "#     result = spm_processor.encode_as_ids(ss)\n",
    "    \n",
    "#     for i  in range(pred_len):\n",
    "#         res_i = next_tokens(result[:input_tokens],lm,1)\n",
    "#         result = result + res_i\n",
    "#     return spm_processor.decode_ids(result)\n",
    "\n",
    "# def get_next_n(inp, n):\n",
    "#     res = inp\n",
    "#     for i in range(n):\n",
    "#         c = get_next(inp)\n",
    "#         res += c\n",
    "#         inp = inp[1:]+c\n",
    "#         if c == '.': break\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_words_good(ss,lm, n_words, toks_at_once=1):\n",
    "    wip = spm_processor.encode_as_ids(ss)\n",
    "    for i  in range(n_words):\n",
    "        ala = wip\n",
    "        wip =  ala + next_tokens(wip,lm,toks_at_once)\n",
    "    return spm_processor.decode_ids(wip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_words_bad(ss,lm, n_words, toks_at_once=1):\n",
    "    wip = ss\n",
    "    for i  in range(n_words):\n",
    "        wip =  wip + \" \"+ next_word(wip,lm,toks_at_once)\n",
    "    return wip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 50000])\n",
      "torch.Size([5, 50000])\n",
      "torch.Size([6, 50000])\n",
      "torch.Size([7, 50000])\n",
      "torch.Size([8, 50000])\n",
      "torch.Size([9, 50000])\n",
      "torch.Size([10, 50000])\n",
      "torch.Size([11, 50000])\n",
      "torch.Size([12, 50000])\n",
      "torch.Size([13, 50000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Kraków to miasto , W W tymi roku mieście w było'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_words_good(\"Kraków to miasto\",lm,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([5, 50000])\n",
      "torch.Size([6, 50000])\n",
      "torch.Size([7, 50000])\n",
      "torch.Size([8, 50000])\n",
      "torch.Size([9, 50000])\n",
      "torch.Size([10, 50000])\n",
      "torch.Size([11, 50000])\n",
      "torch.Size([12, 50000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Matka zrobiła to , co zrobiła , a ja nie wiedziała m'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_words_bad(\"Matka zrobiła\",lm,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BAd en,1\n",
      "torch.Size([5, 50000])\n",
      "torch.Size([6, 50000])\n",
      "torch.Size([7, 50000])\n",
      "torch.Size([8, 50000])\n",
      "torch.Size([9, 50000])\n",
      "torch.Size([11, 50000])\n",
      "torch.Size([12, 50000])\n",
      "torch.Size([13, 50000])\n",
      "torch.Size([14, 50000])\n",
      "torch.Size([15, 50000])\n",
      "torch.Size([16, 50000])\n",
      "torch.Size([17, 50000])\n",
      "torch.Size([18, 50000])\n",
      "torch.Size([19, 50000])\n",
      "torch.Size([20, 50000])\n",
      "torch.Size([21, 50000])\n",
      "torch.Size([22, 50000])\n",
      "torch.Size([23, 50000])\n",
      "torch.Size([23, 50000])\n",
      "torch.Size([23, 50000])\n",
      "torch.Size([23, 50000])\n",
      "torch.Size([23, 50000])\n",
      "torch.Size([23, 50000])\n",
      "torch.Size([23, 50000])\n",
      "torch.Size([23, 50000])\n",
      "torch.Size([23, 50000])\n",
      "torch.Size([23, 50000])\n",
      "torch.Size([23, 50000])\n",
      "torch.Size([23, 50000])\n",
      "torch.Size([23, 50000])\n",
      "Zły dom , w którym mieszka sz o to , że nie ma w nim miejsca na odpoczynek .             \n",
      "\n",
      "Bad 1,en\n",
      "torch.Size([5, 50000])\n",
      "Zły dom ,\n",
      "\n",
      "Good en, 1\n",
      "torch.Size([5, 50000])\n",
      "torch.Size([6, 50000])\n",
      "torch.Size([7, 50000])\n",
      "torch.Size([8, 50000])\n",
      "torch.Size([9, 50000])\n",
      "torch.Size([10, 50000])\n",
      "torch.Size([11, 50000])\n",
      "torch.Size([12, 50000])\n",
      "torch.Size([13, 50000])\n",
      "torch.Size([14, 50000])\n",
      "torch.Size([15, 50000])\n",
      "torch.Size([16, 50000])\n",
      "torch.Size([17, 50000])\n",
      "torch.Size([18, 50000])\n",
      "torch.Size([19, 50000])\n",
      "torch.Size([20, 50000])\n",
      "torch.Size([21, 50000])\n",
      "torch.Size([22, 50000])\n",
      "torch.Size([23, 50000])\n",
      "torch.Size([24, 50000])\n",
      "torch.Size([25, 50000])\n",
      "torch.Size([26, 50000])\n",
      "torch.Size([27, 50000])\n",
      "torch.Size([28, 50000])\n",
      "torch.Size([29, 50000])\n",
      "torch.Size([30, 50000])\n",
      "torch.Size([31, 50000])\n",
      "torch.Size([32, 50000])\n",
      "torch.Size([33, 50000])\n",
      "torch.Size([34, 50000])\n",
      "Zły dom , W W tymi roku mieście w byłoy 5 już tys i ⁇  . . W W związkui z z powyższym\n",
      "\n",
      "Good 1, en\n",
      "torch.Size([5, 50000])\n",
      "Zły dom ,\n",
      "\n",
      "stm\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "Zły dommmmmmmmmmmmmmmmmmmmmmmmmmmmmmm\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Zły dom\"\n",
    "en = 30\n",
    "print(\"\")\n",
    "print(\"BAd en,1\")\n",
    "print(next_words_bad(sentence,lm,en,1))\n",
    "print(\"\")\n",
    "print(\"Bad 1,en\")\n",
    "print(next_words_bad(sentence,lm,1,en)) # poor\n",
    "print(\"\")\n",
    "print(\"Good en, 1\")\n",
    "print(next_words_good(sentence,lm,en,1))\n",
    "print(\"\")\n",
    "print(\"Good 1, en\")\n",
    "print(next_words_good(sentence,lm,1,en)) # poor\n",
    "print(\"\")\n",
    "print(\"stm\")\n",
    "print(next_words_stm(sentence,lm,en,input_tokens= 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50000])\n",
      "torch.Size([2, 50000])\n",
      "torch.Size([2, 50000])\n",
      "torch.Size([2, 50000])\n",
      "torch.Size([2, 50000])\n",
      "torch.Size([2, 50000])\n",
      "torch.Size([2, 50000])\n",
      "torch.Size([2, 50000])\n",
      "torch.Size([2, 50000])\n",
      "torch.Size([2, 50000])\n",
      "torch.Size([2, 50000])\n",
      "torch.Size([2, 50000])\n",
      "torch.Size([2, 50000])\n",
      "torch.Size([2, 50000])\n",
      "torch.Size([2, 50000])\n",
      "torch.Size([2, 50000])\n",
      "torch.Size([2, 50000])\n",
      "torch.Size([2, 50000])\n",
      "torch.Size([2, 50000])\n",
      "torch.Size([2, 50000])\n",
      "Te zabawki należą dożżżżżżżżżżżżżżżżżżżż\n",
      "torch.Size([3, 50000])\n",
      "torch.Size([3, 50000])\n",
      "torch.Size([3, 50000])\n",
      "torch.Size([3, 50000])\n",
      "torch.Size([3, 50000])\n",
      "torch.Size([3, 50000])\n",
      "torch.Size([3, 50000])\n",
      "torch.Size([3, 50000])\n",
      "torch.Size([3, 50000])\n",
      "torch.Size([3, 50000])\n",
      "torch.Size([3, 50000])\n",
      "torch.Size([3, 50000])\n",
      "torch.Size([3, 50000])\n",
      "torch.Size([3, 50000])\n",
      "torch.Size([3, 50000])\n",
      "torch.Size([3, 50000])\n",
      "torch.Size([3, 50000])\n",
      "torch.Size([3, 50000])\n",
      "torch.Size([3, 50000])\n",
      "torch.Size([3, 50000])\n",
      "Te zabawki należą doiiiiiiiiiiiiiiiiiiii\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "torch.Size([4, 50000])\n",
      "Te zabawki należą do są są są są są są są są są są są są są są są są są są są są\n",
      "torch.Size([5, 50000])\n",
      "torch.Size([5, 50000])\n",
      "torch.Size([5, 50000])\n",
      "torch.Size([5, 50000])\n",
      "torch.Size([5, 50000])\n",
      "torch.Size([5, 50000])\n",
      "torch.Size([5, 50000])\n",
      "torch.Size([5, 50000])\n",
      "torch.Size([5, 50000])\n",
      "torch.Size([5, 50000])\n",
      "torch.Size([5, 50000])\n",
      "torch.Size([5, 50000])\n",
      "torch.Size([5, 50000])\n",
      "torch.Size([5, 50000])\n",
      "torch.Size([5, 50000])\n",
      "torch.Size([5, 50000])\n",
      "torch.Size([5, 50000])\n",
      "torch.Size([5, 50000])\n",
      "torch.Size([5, 50000])\n",
      "torch.Size([5, 50000])\n",
      "Te zabawki należą do do do do do do do do do do do do do do do do do do do do do\n",
      "torch.Size([6, 50000])\n",
      "torch.Size([6, 50000])\n",
      "torch.Size([6, 50000])\n",
      "torch.Size([6, 50000])\n",
      "torch.Size([6, 50000])\n",
      "torch.Size([6, 50000])\n",
      "torch.Size([6, 50000])\n",
      "torch.Size([6, 50000])\n",
      "torch.Size([6, 50000])\n",
      "torch.Size([6, 50000])\n",
      "torch.Size([6, 50000])\n",
      "torch.Size([6, 50000])\n",
      "torch.Size([6, 50000])\n",
      "torch.Size([6, 50000])\n",
      "torch.Size([6, 50000])\n",
      "torch.Size([6, 50000])\n",
      "torch.Size([6, 50000])\n",
      "torch.Size([6, 50000])\n",
      "torch.Size([6, 50000])\n",
      "torch.Size([6, 50000])\n",
      "Te zabawki należą do najbardziej najbardziej najbardziej najbardziej najbardziej najbardziej najbardziej najbardziej najbardziej najbardziej najbardziej najbardziej najbardziej najbardziej najbardziej najbardziej najbardziej najbardziej najbardziej najbardziej\n",
      "torch.Size([6, 50000])\n",
      "torch.Size([7, 50000])\n",
      "torch.Size([7, 50000])\n",
      "torch.Size([7, 50000])\n",
      "torch.Size([7, 50000])\n",
      "torch.Size([7, 50000])\n",
      "torch.Size([7, 50000])\n",
      "torch.Size([7, 50000])\n",
      "torch.Size([7, 50000])\n",
      "torch.Size([7, 50000])\n",
      "torch.Size([7, 50000])\n",
      "torch.Size([7, 50000])\n",
      "torch.Size([7, 50000])\n",
      "torch.Size([7, 50000])\n",
      "torch.Size([7, 50000])\n",
      "torch.Size([7, 50000])\n",
      "torch.Size([7, 50000])\n",
      "torch.Size([7, 50000])\n",
      "torch.Size([7, 50000])\n",
      "torch.Size([7, 50000])\n",
      "Te zabawki należą do najbardziej\n",
      "torch.Size([6, 50000])\n",
      "torch.Size([7, 50000])\n",
      "torch.Size([8, 50000])\n",
      "torch.Size([8, 50000])\n",
      "torch.Size([8, 50000])\n",
      "torch.Size([8, 50000])\n",
      "torch.Size([8, 50000])\n",
      "torch.Size([8, 50000])\n",
      "torch.Size([8, 50000])\n",
      "torch.Size([8, 50000])\n",
      "torch.Size([8, 50000])\n",
      "torch.Size([8, 50000])\n",
      "torch.Size([8, 50000])\n",
      "torch.Size([8, 50000])\n",
      "torch.Size([8, 50000])\n",
      "torch.Size([8, 50000])\n",
      "torch.Size([8, 50000])\n",
      "torch.Size([8, 50000])\n",
      "torch.Size([8, 50000])\n",
      "torch.Size([8, 50000])\n",
      "Te zabawki należą do najbardziej W W W W W W W W W W W W W W W W W W\n"
     ]
    }
   ],
   "source": [
    "for i in range(3,10):\n",
    "    print(next_words_stm(\"Te zabawki należą do\",lm,20,input_tokens=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([    2,   618,     8,    91,   431, 11111,   344, 30517,     3])]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inpucik = \"Mama bardzo dobrze wydała swoje ulubione \"\n",
    "test_ids = [np.array(spm_processor.encode_as_ids(inpucik))]\n",
    "test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mama bardzo dobrze wydała swoje ulubione'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = LMTextDataset(test_ids)\n",
    "test_samp = SortSampler(test_ids, key=lambda x: len(test_ids[x]))\n",
    "test_dl = DataLoader(test_ds, bs, transpose=True, transpose_y=True, num_workers=1, pad_idx=PAD_ID, sampler=test_samp, pre_pad=False)\n",
    "x,y = list(test_dl)[0]\n",
    "x2 = [int(i) for i in x]\n",
    "# y2 = [int(i) for i in y]\n",
    "spm_processor.decode_ids(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getp():\n",
    "    dl = test_dl\n",
    "    loss = 0.0\n",
    "    tensor1 = None\n",
    "    lm.reset()\n",
    "    with no_grad_context():\n",
    "        for (x, y) in tqdm(dl):\n",
    "            targets = y.view(-1)\n",
    "            preds = lm(x)[0]\n",
    "            tensor1= lm(x)\n",
    "            not_pads = targets != PAD_ID\n",
    "            ce = F.cross_entropy(preds[not_pads], targets[not_pads], reduction='sum')\n",
    "            loss += ce\n",
    "    loss\n",
    "    p= tensor1[0]\n",
    "    print(p[0].size())\n",
    "    print ([type(i) for i in tensor1])\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(n):\n",
    "    p = getp()\n",
    "    print(p.size())\n",
    "    \n",
    "#     for pi in p:\n",
    "#         r = torch.multinomial(pi.exp(), n)\n",
    "#         predicted_ids = [int(r_) for r_ in r]\n",
    "#         print(spm_processor.decode_ids(predicted_ids))\n",
    "#         print(\"\")\n",
    "#         print(\"\")\n",
    "\n",
    "    r = torch.multinomial(p[-1].exp(), 10)\n",
    "    print(r)\n",
    "    print([p[-1][i] for i in r])\n",
    "    predicted_ids = [int(r_) for r_ in r]\n",
    "    return spm_processor.decode_ids(predicted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mama bardzo dobrze wydała swoje ulubione '"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inpucik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14.31it/s]\n",
      "torch.Size([50000])\n",
      "[<class 'torch.Tensor'>, <class 'list'>, <class 'list'>]\n"
     ]
    }
   ],
   "source": [
    "p = getp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functional import seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4838, 4.52532958984375]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq(i for i in p[-1]).map(lambda x: float(x)).enumerate().max_by(lambda id_val: id_val[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'piosenki'"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm_processor.decode_ids([4838])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.97it/s]\n",
      "torch.Size([50000])\n",
      "[<class 'torch.Tensor'>, <class 'list'>, <class 'list'>]\n",
      "torch.Size([7, 50000])\n",
      "tensor([48758, 47070, 15515, 38977, 49387, 24972,  8005,  3566, 10795, 17061],\n",
      "       device='cuda:0')\n",
      "[tensor(-0.4503, device='cuda:0'), tensor(-0.5835, device='cuda:0'), tensor(-0.0726, device='cuda:0'), tensor(-2.5826, device='cuda:0'), tensor(-0.7917, device='cuda:0'), tensor(2.1340, device='cuda:0'), tensor(1.7828, device='cuda:0'), tensor(-4.4941, device='cuda:0'), tensor(-0.9323, device='cuda:0'), tensor(-0.6776, device='cuda:0')]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-215-d456e3d52732>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/git/studia/nlp/lab10/blobs.bak/fork_venv/lib/python3.6/site-packages/torch-1.1.0-py3.6-linux-x86_64.egg/torch/tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "get_prediction(20)\n",
    "np.array(p[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm.predict(text=\"ala ma kota\", n_words=10, temperature=0.75) works in newer version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dark  place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "from fastai import learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-8a8da1601bd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/git/studia/nlp/lab10/blobs.bak/fork_venv/lib/python3.6/site-packages/fastai-0.7.0-py3.6.egg/fastai/torch_imports.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(m, p)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0msd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# list \"detatches\" the iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/studia/nlp/lab10/blobs.bak/fork_venv/lib/python3.6/site-packages/torch-1.1.0-py3.6-linux-x86_64.egg/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "learner.load_model(lm,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'md' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-30fa863f8743>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdrops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m learner= md.get_model(opt_fn, em_sz, nh, nl, \n\u001b[0m\u001b[1;32m     10\u001b[0m     dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'md' is not defined"
     ]
    }
   ],
   "source": [
    "wd=1e-7\n",
    "bptt=70\n",
    "bs=52\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "\n",
    "\n",
    "drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7\n",
    "\n",
    "learner= md.get_model(opt_fn, em_sz, nh, nl, \n",
    "    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])\n",
    "\n",
    "learner.metrics = [accuracy]\n",
    "learner.freeze_to(-1)\n",
    "\n",
    "learner.model.load_state_dict(wgts)\n",
    "\n",
    "lr=1e-2\n",
    "lrs = lr\n",
    "\n",
    "learner\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# old things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from fastai.torch_core import *\n",
    "from fastai.basic_data import *\n",
    "from fastai.callback import *\n",
    "from fastai.data_block import *\n",
    "from fastai.utils.ipython import gpu_mem_restore\n",
    "import inspect\n",
    "from fastprogress.fastprogress import format_time, IN_NOTEBOOK\n",
    "from time import time\n",
    "from fastai.sixel import plot_sixel\n",
    "\n",
    "def load_that_learner(torch_thing,**db_kwargs):\n",
    "    \"Load a `Learner` object saved with `export_state` in `path/file` with empty data, optionally add `test` and load on `cpu`. `file` can be file-like (file or buffer)\"\n",
    "    state = torch_thing\n",
    "    model = state.pop('model')\n",
    "    src = LabelLists.load_state(path, state.pop('data'))\n",
    "    if test is not None: src.add_test(test)\n",
    "    data = src.databunch(**db_kwargs)\n",
    "    cb_state = state.pop('cb_state')\n",
    "    clas_func = state.pop('cls')\n",
    "    res = clas_func(data, model, **state)\n",
    "    res.callback_fns = state['callback_fns'] #to avoid duplicates\n",
    "    res.callbacks = [load_callback(c,s, res) for c,s in cb_state.items()]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "load_that_learner(wgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "LM_PATH = \"./blobs.bak/work/up_low50k/models\"\n",
    "LM_FILE = \"fwd_v50k_finetune_lm_enc.h5\"\n",
    "model = load_learner(LM_PATH,LM_FILE,load_on='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "help(load_learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "help(get_language_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fork_venv",
   "language": "python",
   "name": "fork_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
