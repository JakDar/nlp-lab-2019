{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!#git clone -b poleval/task3 https://github.com/n-waves/fastai --depth 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T08:46:44.401958Z",
     "start_time": "2019-05-20T08:46:44.398089Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from functional import seq\n",
    "from IPython.core.display import HTML\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T08:45:09.261251Z",
     "start_time": "2019-05-20T08:45:09.258254Z"
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mblobs.bak/work/up_low50k/\u001b[00m\r\n",
      "├── \u001b[01;34mmodels\u001b[00m\r\n",
      "│   └── fwd_v50k_finetune_lm_enc.h5\r\n",
      "└── \u001b[01;34mtmp\u001b[00m\r\n",
      "    ├── sp-50k.model\r\n",
      "    └── sp-50k.vocab\r\n",
      "\r\n",
      "2 directories, 3 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree blobs.bak/work/up_low50k/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai_model_path = \"./blobs.bak/work/up_low50k/models/fwd_v50k_finetune_lm_enc.h5\"\n",
    "sentencepiece_model_path = \"./blobs.bak/work/up_low50k/tmp/sp-50k.model\"\n",
    "sentencepiece_vocab_path = \"./blobs.bak/work/up_low50k/tmp/sp-50k.vocab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists_or_ex(path):\n",
    "    file = open(path,\"r\")\n",
    "    file.close()\n",
    "_ = [exists_or_ex(path) for path in [fastai_model_path, sentencepiece_model_path, sentencepiece_vocab_path]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coloring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "\n",
    "def highlighted(\n",
    "        highlight,\n",
    "        whole_text,\n",
    "):\n",
    "    return whole_text.replace(\n",
    "        highlight, colored(highlight, color=\"green\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "spm_processor = spm.SentencePieceProcessor()\n",
    "spm_processor.Load(sentencepiece_model_path)\n",
    "\n",
    "spm_processor.SetEncodeExtraOptions(\"bos:eos\")\n",
    "spm_processor.SetDecodeExtraOptions(\"bos:eos\")\n",
    "# spm_processor.LoadVocabulary(sentencepiece_vocab_path,threshold= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lm(bptt, max_seq, n_tok, emb_sz, n_hid, n_layers, pad_token, bidir=False,\n",
    "           tie_weights=True, qrnn=False):\n",
    "    rnn_enc = MultiBatchRNN(bptt, max_seq, n_tok, emb_sz, n_hid, n_layers, pad_token=pad_token, bidir=bidir, qrnn=qrnn)\n",
    "    enc = rnn_enc.encoder if tie_weights else None\n",
    "    return SequentialRNN(rnn_enc, LinearDecoder(n_tok, emb_sz, 0, tie_encoder=enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): MultiBatchRNN(\n",
       "    (encoder): Embedding(50000, 400, padding_idx=1)\n",
       "    (encoder_with_dropout): EmbeddingDropout(\n",
       "      (embed): Embedding(50000, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDrop(\n",
       "        (module): LSTM(400, 1150)\n",
       "      )\n",
       "      (1): WeightDrop(\n",
       "        (module): LSTM(1150, 1150)\n",
       "      )\n",
       "      (2): WeightDrop(\n",
       "        (module): LSTM(1150, 1150)\n",
       "      )\n",
       "      (3): WeightDrop(\n",
       "        (module): LSTM(1150, 400)\n",
       "      )\n",
       "    )\n",
       "    (dropouti): LockedDropout()\n",
       "    (dropouths): ModuleList(\n",
       "      (0): LockedDropout()\n",
       "      (1): LockedDropout()\n",
       "      (2): LockedDropout()\n",
       "      (3): LockedDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=50000, bias=False)\n",
       "    (dropout): LockedDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.set_device(0)\n",
    "\n",
    "UNK_ID = 0\n",
    "PAD_ID = 1\n",
    "BOS_ID = 2\n",
    "EOS_ID = 3\n",
    "UP_ID  = 4\n",
    "bs=22\n",
    "\n",
    "em_sz,nh,nl = 400 ,1150,4\n",
    "\n",
    "bptt=5\n",
    "vs = len(spm_processor)\n",
    "\n",
    "lm = get_lm(bptt, 1000000, vs, em_sz, nh, nl, PAD_ID)\n",
    "lm = to_gpu(lm)\n",
    "load_model(lm[0],fastai_model_path)\n",
    "lm.reset()\n",
    "lm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMTextDataset(Dataset):\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.x[idx]\n",
    "        return sentence[:-1], sentence[1:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_tokens(ids_, model, toks_at_once):  #toks_at_once wont be used\n",
    "    ids = [np.array(ids_)]\n",
    "    test_ds = LMTextDataset(ids)\n",
    "    test_samp = SortSampler(ids, key=lambda x: len(ids[x]))\n",
    "    dl = DataLoader(test_ds,\n",
    "                    bs,\n",
    "                    transpose=True,\n",
    "                    transpose_y=True,\n",
    "                    num_workers=1,\n",
    "                    pad_idx=PAD_ID,\n",
    "                    sampler=test_samp,\n",
    "                    pre_pad=False)\n",
    "\n",
    "    tensor1 = None\n",
    "    model.reset()  # todo:bcm - do or dont'?\n",
    "    with no_grad_context():\n",
    "        for (x, y) in dl:\n",
    "            tensor1 = model(x)\n",
    "    p = tensor1[0]\n",
    "    \n",
    "    #     arg = torch.sum(p[:-2],0)\n",
    "    arg =  p[-1]\n",
    "#     arg, _  = torch.max(p,dim=0)\n",
    "#     print(arg.size())\n",
    "    r = int(torch.argmax(arg))\n",
    "#     r = int(torch.multinomial(p[-1].exp(), 1))\n",
    "\n",
    "\n",
    "    while r in [ids_[-1],24]: #, BOS_ID,EOS_ID, UNK_ID]:\n",
    "        arg[r] = -1\n",
    "        r = int(torch.argmax(arg))\n",
    "\n",
    "    predicted_ids = [r]\n",
    "    return predicted_ids\n",
    "\n",
    "\n",
    "def next_word(ss, model, toks_at_once):\n",
    "    ids = spm_processor.encode_as_ids(ss)\n",
    "    return spm_processor.decode_ids(next_tokens(ids, model, toks_at_once))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_words_good(ss, lm, n_words, finishers=set([UNK_ID])):\n",
    "    initial_wip = spm_processor.encode_as_ids(ss)\n",
    "    wip = initial_wip\n",
    "    for i in range(n_words):\n",
    "        wip = wip + next_tokens(wip, lm, 1)\n",
    "\n",
    "    print(seq(wip).drop(len(initial_wip)))\n",
    "    return spm_processor.decode_ids(wip)\n",
    "#         seq(wip).take_while(lambda x: x not in finishers).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_words_bad(ss, lm, n_words, toks_at_once=1):\n",
    "    wip = ss\n",
    "    for i in range(n_words):\n",
    "        wip = wip + \" \" + next_word(wip, lm, toks_at_once)\n",
    "    return wip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm_processor.decode_ids([7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Warszawa to największe\", \"Te zabawki należą do\",\n",
    "    \"Policjant przygląda się\", \"Na środku skrzyżowania widać\",\n",
    "    \"Właściciel samochodu widział złodzieja z\",\n",
    "    \"Prezydent z premierem rozmawiali wczoraj o\", \"Witaj drogi\",\n",
    "    \"Gdybym wiedział wtedy dokładnie to co wiem teraz, to bym się nie\",\n",
    "    \"Gdybym wiedziała wtedy dokładnie to co wiem teraz, to bym się nie\",\n",
    "    \"Polscy naukowcy odkryli w Tatrach nowy gatunek istoty żywej. Zwięrzę to przypomina małpę, lecz porusza się na dwóch nogach i potrafi posługiwać się narzędziami. Przy dłuższej obserwacji okazało się, że potrafi również posługiwać się językiem polskim, a konkretnie gwarą podhalańską. Zwierzę to zostało nazwane\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1045, 2, 61, 73, 369, 41397, 4, 11274, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7]\n",
      "\u001b[32mWarszawa to największe\u001b[0m miasto Po Nanadtowroc ,kiej  , ⁇   , ⁇   , ⁇   , ⁇   , ⁇   , ⁇   , ⁇  \n",
      "\u001b[32mWarszawa to największe\u001b[0m miasto w Polsce , które jest największym miastem w Polsce . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "[558, 2, 61, 73, 369, 41397, 4, 11274, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7]\n",
      "\u001b[32mTe zabawki należą do\u001b[0m najbardziej Po Nanadtowroc ,kiej  , ⁇   , ⁇   , ⁇   , ⁇   , ⁇   , ⁇   , ⁇  \n",
      "\u001b[32mTe zabawki należą do\u001b[0m najbardziej popularnych i najbardziej popularnych w Polsce . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "[4, 2, 61, 73, 369, 41397, 4, 11274, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7]\n",
      "\u001b[32mPolicjant przygląda się\u001b[0m , Po Nanadtowroc ,kiej  , ⁇   , ⁇   , ⁇   , ⁇   , ⁇   , ⁇   , ⁇  \n",
      "\u001b[32mPolicjant przygląda się\u001b[0m , jak a ntyterrorystyczn y                         \n",
      "\n",
      "[2973, 2, 61, 73, 369, 41397, 4, 11274, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7]\n",
      "\u001b[32mNa środku skrzyżowania widać\u001b[0m ślad Po Nanadtowroc ,kiej  , ⁇   , ⁇   , ⁇   , ⁇   , ⁇   , ⁇   , ⁇  \n",
      "\u001b[32mNa środku skrzyżowania widać\u001b[0m ślad y a w nim ślad y . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "[7, 2, 0, 61, 7, 27038, 0, 6, 7, 624, 0, 4, 7, 6, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0]\n",
      "\u001b[32mWłaściciel samochodu widział złodzieja z\u001b[0m  ⁇  Po kój ⁇  w  domu ⁇  ,  w ⁇   , ⁇   , ⁇   , ⁇   , ⁇   , ⁇ \n",
      "\u001b[32mWłaściciel samochodu widział złodzieja z\u001b[0m                              \n",
      "\n",
      "[49, 2, 61, 73, 369, 41397, 4, 11274, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7]\n",
      "\u001b[32mPrezydent z premierem rozmawiali wczoraj o\u001b[0m tym Po Nanadtowroc ,kiej  , ⁇   , ⁇   , ⁇   , ⁇   , ⁇   , ⁇   , ⁇  \n",
      "\u001b[32mPrezydent z premierem rozmawiali wczoraj o\u001b[0m tym , że w Polsce jest tak , że w Polsce jest ok . . . . . . . . . . . . . . . . .\n",
      "\n",
      "[4, 2, 61, 73, 369, 41397, 4, 11274, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7]\n",
      "\u001b[32mWitaj drogi\u001b[0m , Po Nanadtowroc ,kiej  , ⁇   , ⁇   , ⁇   , ⁇   , ⁇   , ⁇   , ⁇  \n",
      "\u001b[32mWitaj drogi\u001b[0m , nie                            \n",
      "\n",
      "[22542, 2, 61, 73, 369, 41397, 4, 11274, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7]\n",
      "\u001b[32mGdybym wiedział wtedy dokładnie to co wiem teraz, to bym się nie\u001b[0m spodziewał Po Nanadtowroc ,kiej  , ⁇   , ⁇   , ⁇   , ⁇   , ⁇   , ⁇   , ⁇  \n",
      "\u001b[32mGdybym wiedział wtedy dokładnie to co wiem teraz, to bym się nie\u001b[0m spodziewał , że to nie jest tak , że nie ma co się bać . . . . . . . . . . . . . . . .\n",
      "\n",
      "[22542, 2, 61, 73, 369, 41397, 4, 11274, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7]\n",
      "\u001b[32mGdybym wiedziała wtedy dokładnie to co wiem teraz, to bym się nie\u001b[0m spodziewał Po Nanadtowroc ,kiej  , ⁇   , ⁇   , ⁇   , ⁇   , ⁇   , ⁇   , ⁇  \n",
      "\u001b[32mGdybym wiedziała wtedy dokładnie to co wiem teraz, to bym się nie\u001b[0m spodziewał a nie wiem jak to zrobić . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "[12, 2, 61, 73, 369, 41397, 4, 11274, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7, 4, 0, 7]\n",
      "\u001b[32mPolscy naukowcy odkryli w Tatrach nowy gatunek istoty żywej. Zwięrzę to przypomina małpę, lecz porusza się na dwóch nogach i potrafi posługiwać się narzędziami. Przy dłuższej obserwacji okazało się, że potrafi również posługiwać się językiem polskim, a konkretnie gwarą podhalańską. Zwierzę to zostało nazwane\u001b[0m na Po Nanadtowroc ,kiej  , ⁇   , ⁇   , ⁇   , ⁇   , ⁇   , ⁇   , ⁇  \n",
      "\u001b[32mPolscy naukowcy odkryli w Tatrach nowy gatunek istoty żywej. Zwięrzę to przypomina małpę, lecz porusza się na dwóch nogach i potrafi posługiwać się narzędziami. Przy dłuższej obserwacji okazało się, że potrafi również posługiwać się językiem polskim, a konkretnie gwarą podhalańską. Zwierzę to zostało nazwane\u001b[0m na cześć                            \n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(\"\")\n",
    "    lm.reset()\n",
    "    print(highlighted(sentence,next_words_good(sentence, lm, 30)))\n",
    "    lm.reset()\n",
    "    print(highlighted(sentence,next_words_bad(sentence, lm, 30)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([    2,   618,     8,    91,   431, 11111,   344, 30517,     3])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inpucik = \"Mama bardzo dobrze wydała swoje ulubione \"\n",
    "test_ids = [np.array(spm_processor.encode_as_ids(inpucik))]\n",
    "test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mama bardzo dobrze wydała swoje ulubione'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = LMTextDataset(test_ids)\n",
    "test_samp = SortSampler(test_ids, key=lambda x: len(test_ids[x]))\n",
    "test_dl = DataLoader(test_ds,\n",
    "                     bs,\n",
    "                     transpose=True,\n",
    "                     transpose_y=True,\n",
    "                     num_workers=1,\n",
    "                     pad_idx=PAD_ID,\n",
    "                     sampler=test_samp,\n",
    "                     pre_pad=False)\n",
    "x, y = list(test_dl)[0]\n",
    "x2 = [int(i) for i in x]\n",
    "# y2 = [int(i) for i in y]\n",
    "spm_processor.decode_ids(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getp():\n",
    "    dl = test_dl\n",
    "    loss = 0.0\n",
    "    tensor1 = None\n",
    "    lm.reset()\n",
    "    with no_grad_context():\n",
    "        for (x, y) in tqdm(dl):\n",
    "            targets = y.view(-1)\n",
    "            preds = lm(x)[0]\n",
    "            tensor1 = lm(x)\n",
    "            not_pads = targets != PAD_ID\n",
    "            ce = F.cross_entropy(preds[not_pads],\n",
    "                                 targets[not_pads],\n",
    "                                 reduction='sum')\n",
    "            loss += ce\n",
    "    loss\n",
    "    p = tensor1[0]\n",
    "    print(p[0].size())\n",
    "    print([type(i) for i in tensor1])\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_prediction(n):\n",
    "    p = getp()\n",
    "    print(p.size())\n",
    "\n",
    "    #     for pi in p:\n",
    "    #         r = torch.multinomial(pi.exp(), n)\n",
    "    #         predicted_ids = [int(r_) for r_ in r]\n",
    "    #         print(spm_processor.decode_ids(predicted_ids))\n",
    "    #         print(\"\")\n",
    "    #         print(\"\")\n",
    "\n",
    "    r = torch.multinomial(p[-1].exp(), 10)\n",
    "    print(r)\n",
    "    print([p[-1][i] for i in r])\n",
    "    predicted_ids = [int(r_) for r_ in r]\n",
    "    return spm_processor.decode_ids(predicted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mama bardzo dobrze wydała swoje ulubione '"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inpucik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14.31it/s]\n",
      "torch.Size([50000])\n",
      "[<class 'torch.Tensor'>, <class 'list'>, <class 'list'>]\n"
     ]
    }
   ],
   "source": [
    "p = getp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from functional import seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4838, 4.52532958984375]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq(i for i in\n",
    "    p[-1]).map(lambda x: float(x)).enumerate().max_by(lambda id_val: id_val[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'piosenki'"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm_processor.decode_ids([4838])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.97it/s]\n",
      "torch.Size([50000])\n",
      "[<class 'torch.Tensor'>, <class 'list'>, <class 'list'>]\n",
      "torch.Size([7, 50000])\n",
      "tensor([48758, 47070, 15515, 38977, 49387, 24972,  8005,  3566, 10795, 17061],\n",
      "       device='cuda:0')\n",
      "[tensor(-0.4503, device='cuda:0'), tensor(-0.5835, device='cuda:0'), tensor(-0.0726, device='cuda:0'), tensor(-2.5826, device='cuda:0'), tensor(-0.7917, device='cuda:0'), tensor(2.1340, device='cuda:0'), tensor(1.7828, device='cuda:0'), tensor(-4.4941, device='cuda:0'), tensor(-0.9323, device='cuda:0'), tensor(-0.6776, device='cuda:0')]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-215-d456e3d52732>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/git/studia/nlp/lab10/blobs.bak/fork_venv/lib/python3.6/site-packages/torch-1.1.0-py3.6-linux-x86_64.egg/torch/tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "get_prediction(20)\n",
    "np.array(p[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fork_venv",
   "language": "python",
   "name": "fork_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
